\section{An exocentric vision system for Morduc}
\label{sec:concr}
This section will show how to use \textsf{REAR} to implement a 
concrete exocentric vision system, specifically designed for 
Morduc.
%

%
As section \ref{sec:rear} explains, in order to get things 
done, all we need to write is:
\begin{itemize}
\item a concrete subclass of \texttt{Robot}
\item an implementation of the \texttt{IDataLogic} interface
\item an implementation of the \texttt{IImageSelector} interface
\end{itemize}
%
The class diagram of the complete system is reported in figure 
\ref{fig:class_diagram_complete}. 
%
As it's easy to notice, it features two implementations 
of the \texttt{IImageSelector} interface: \texttt{SpacialMetricCalc} 
and \texttt{SweepMetricCalc}. The former is nothing more than 
a mere implementation of the image selection method 2 reported in 
\cite{sugimoto} and already introduced in section \ref{sub:iimageselector}.
%

%
\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=400pt]{img/class_diagram.png} 
    \caption{Class diagram of the whole system}
    \label{fig:class_diagram_complete}
  \end{center}
\end{figure}
%

The latter, instead, encapsulates a brand new selection algorithm. 
It will be widely discussed later in section 
\ref{subsec:sweep_metric_algorithm}.


\subsection{The Morduc Class}
\label{subsec:morducclass}

The \texttt{Morduc} class publicly inherits from \texttt{Robot} 
and, hence, implements the \texttt{DrawRobot()} method:
%
\begin{lstlisting}[caption={\texttt{Morduc} class declaration}, label={code:morducclass}, frame=trBL]
class Morduc : public Robot
{
 private:
  GLfloat radius;
  void PaintCylinder(GLfloat radius,GLfloat height);
  void PaintDisk(GLfloat radius);

 public:
  Morduc(float radius = 4.0f);
  void DrawRobot();
};
\end{lstlisting}
%
Such a method, reported in listing \ref{code:drawrobot}, contains the 
procedure to actually draw the OpenGL model representing Murdoc 
(in reality, Murdoc looks like a stack of three disks linked 
by thin cylinders - see figure \ref{fig:3morduc_opengl}).
%
\begin{lstlisting}[caption={\texttt{Morduc::DrawRobot()} function}, label={code:drawrobot}, frame=trBL]
void Morduc::DrawRobot()
{
  GLfloat reflectance_black[] = { 0.2f, 0.2f, 0.2f};
  GLfloat reflectance_white[] = { 0.8f, 0.8f, 0.8f};
  
  glMatrixMode(GL_MODELVIEW);  
  glPushMatrix();

  // set robot reflectance (it is black)
  glMaterialfv(GL_FRONT_AND_BACK, GL_AMBIENT, 
               reflectance_black);

  // set robot position
  glTranslatef(this -> x, 0.0f, this -> y);

  glRotatef(this -> theta, 0.0f, 1.0f, 0.0f);
  
  // translate on z axis
  // not to make the robot wipe the floor
  glTranslatef(0.0f,0.08f,0.0f);

  glScalef(radius, radius, radius);
  
  // draw robot
  PaintCylinder(1.0f, 0.1);
  PaintDisk(-1.0f);
  glTranslatef(0.0f, 0.1f, 0.0f);
  PaintDisk(1.0f);
  
  glTranslatef(0.0f, 0.6f, 0.0f);

  PaintCylinder(1.0f, 0.1f);
  PaintDisk(-1.0f);
  glTranslatef(0.0f, 0.1f, 0.0f);
  PaintDisk(1.0f);

  glTranslatef(0.8f, 0.0f, 0.0f);
  glColor3f(0.5f, 0.5f, 0.5f);
  PaintCylinder(0.2f, 0.3f);
  glTranslatef(0.0f, 0.3f, 0.0f);
  PaintDisk(0.2f);

  glTranslatef(0,0.401,0);
  glMaterialfv(GL_FRONT_AND_BACK, GL_AMBIENT, 
               reflectance_white);
  PaintDisk(0.1f);
  glTranslatef(0,-0.701,0);
  glMaterialfv(GL_FRONT_AND_BACK, GL_AMBIENT, 
               reflectance_black);

  glTranslatef(-0.8f, 0.0f, 0.0f);
  glColor3f(0.1f, 0.1f, 0.1f);
  glTranslatef(0.0f ,0.6f, 0.0f);
  PaintCylinder(1.0f, 0.1f);
  PaintDisk(-1.0f);
  glTranslatef(0.0f, 0.1f, 0.0f); 
  PaintDisk(1.0f);

  glTranslatef(0.0f, -1.5f, 0.0f);
  glTranslatef(0.0f, 0.0f, 0.8f);
  PaintCylinder(0.1f, 1.5f);
  glTranslatef(0.0f, 0.0f, -1.60f);
  PaintCylinder(0.1f, 1.5f);
  glTranslatef(-0.8f, 0.0f, 0.8f);

  PaintCylinder(0.1f, 1.5f);
  glTranslatef(0.8f, 0.0f, 0.0f);

  glScalef(1/radius, 1/radius, 1/radius);

  glPopMatrix();
}
\end{lstlisting}
%
Notice how such a function implements the skeleton 
presented in listing \ref{code:drawrobot_skeleton}.
%

%
As you have surely noticed, \texttt{Morduc} contains also 
some private attributes, precisely two methods - 
\texttt{PaintCylinder()} and \texttt{PaintDisk()} - and 
a float field - \texttt{radius}.
%
The two methods are just \textit{helper functions}, that is, 
functions that contains a procedure to draw, respectively,  
a \textit{disk} and a \textit{cylinder}.
%
\texttt{radius}, instead, stores the value of the radius 
for the of the cylinders which make up the robot.
%
\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=200pt]{img/3morduc_opengl.png}
    \caption{The 3morduc robotic platform drawn in OpenGL}
    \label{fig:3morduc_opengl}
  \end{center}
\end{figure}
%
After some calibration, the default \texttt{radius} value 
has been set to 4.0.
%

%
\subsection{Data source}
\label{sub:datasource}
For the reasons explained in section \ref{sec:simulator}, 
we used a Morduc simulator in order to generate and record 
test data.



This chapter will show a concrete implementation of 
\texttt{IDataLogic}, created to use the log files data written
in previously runs of Morduc simulations (see chapter \ref{sec:simulator}).


%

%
Every simulation is identified by a number of session. It contains several
images files (the egocentric robot vision) and one textual file, filled with the
odometer values. For more information about its format, see again chapter
\ref{sec:simulator}.
%

%
The concrete class \texttt{DataLogic} presumes to find logs data in a prefixed
path, since the latter is included in the source code. In order to execute the
application, log data must be saved in the \texttt{log} upper-level folder (compared
to the execution path).
%

%
\texttt{log} folder must contains a single folder for every logged session, named
\texttt{log\_$\langle$number of session$\rangle$}. Within this, the images files and the
textual one are stored.
%

%
To implement \texttt{IDataLogic}, \texttt{DataLogic} must define three methods (for more
details see chapter \ref{sub:idatalogic}):
:
%
\begin{itemize}

  \item \texttt{Command\{int \}} \\
    \texttt{int} specifies where robot must move 

  \item \texttt{RetrieveData\{robot\_data *\}} \\
    \texttt{robot\_data} will be used to return new robot status

  \item \texttt{SelectImage\{robot\_data *, image\_data *, IImageSelector *\}} \\
    \texttt{robot\_data} points to actual robot status; \texttt{image\_data} will be used
    to return the chosen image; \texttt{ImageSelector} points to the class where the image
    selection algorithm is defined.


\end{itemize}
%

Since the \texttt{DataLogic} class reads from a textual file robot status (i.e. orientation
and position), we can not decide the direction in which the robot moves, because a previous
path has been already covered. For this reason the \texttt{Command} will not take into account
the received command, but will simply increment an internal counter used to know which is the next
line to read in the textual file (we remember that every line contains data about robot status).
%

%
Different implementation of \texttt{IDataLogic} can instead exploits the \texttt{int} parameters
in order to send the right command to the robot o the simulator, if the interaction is in real time
(see chapter \ref{sec:future_works} for more details).
%

%
The \texttt{RetrieveData} method allows to read the new robot status from the session textual file
and fill the \texttt{robot\_data} pointed struct with the right values. Besides, it adds a new
\texttt{image\_data} struct to its set, in this case implemented with a \texttt{std::vector} instance.
The struct added represents the image shoot by the robot in the new position (the egocentric
vision).
%

%
Finally, the \texttt{SelectImage} method calls \texttt{ChooseImage} method on a con created instance
of \texttt{IImageSelector} interface, thanks to the given reference in input. For further details
about \texttt{ChooseImage} method see chapter \ref{sub:iimageselector}.
%

%
Because the \texttt{ChooseImage} takes a pointer to the actual robot status and an
\texttt{image\_data} pointer used to return the chosen image, the main difference lies in the third (and last)
parameter. The concrete \texttt{ChooseImage} requests an \texttt{std::vector} object with all the image stored,
whereas a generic \texttt{IDataLogic} implementation can use a different type of set. In this case, the
\texttt{ChooseImage} method must convert the generic set containing \texttt{image\_data} in to an \texttt{std::vector},
before calling the \texttt{ChooseImagem} method. Finally, the retrieved image will be then returned.
%



\subsection{Metrics}
\label{sub:metrics}

This chapter will introduce two algorithms implemented to choose the
background image, by defining the \texttt{ChooseImage} function (see
chapter \ref{sub:iimageselector}).

%

%
Both algorithms use a \texttt{Calculate} function to assign a score
value for each image, by knowing the robot status and a single image.
%

%
After assigning a score for every collected image, the one coupled with the
lower score will be returned (or better, assigned to the \texttt{image\_data}
pointer passed to \texttt{ChooseImage} method).
%

%
Both algorithms take its stand on a specific metric, expressed by the \texttt{Calculate}
method.
%

%
In our case, a metric or distance function is a function which defines a
distance between elements belonging to two different and separate sets. The
first one contains the robot status elements, the second one the egocentric images
shot by the robot. Both types of elements are represented, in practice, by two structs
in C++ (see diagram in figure \ref{fig:class_diagram}).
%

%
The \texttt{Calculate} function returns a score (or distance) value associated with a robot
status and a single image. The former is retrieved directly from the \texttt{ChooseImage} 
method, the latter by iterating the image collection parameter.
%

%
Even though the metric function is able to work with every couple of input
data, the two concrete instances of \texttt{DistanceCalcInterface} will call
the \texttt{Calculate} method for every stored images, but always with the
same robot status. In this way it is possible to find which is the closer (according to the
chosen metric) image to a specific robot status, by selecting the image coupled with the lowest
score.
%

%
For further details, we refer to chapter \ref{sub:iimageselector}.




\subsubsection{The spacial metric algorithm}
\label{subsec:spacial_metric_algorithm}

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=400pt]{img/spacialMetricFunc.png} 
    \caption{Spacial Metric Function}
    \label{fig:spacial_metric_func}
  \end{center}
\end{figure}

This algorithm uses a simple triangle function to couple the image processed with a score.
%

%
The euclidean distance between robot and image is processed by a triangle function. User specifies
the optimal distance between image and robot, that is the distance value where
the score function returns the maximum tally and therefore where the triangle is centred.
In figure \ref{fig:spacial_metric_func} the optimal distance is set to five.
%

%
Since the \texttt{ChooseImage} method will select the image coupled with the minimum value, we have
to invert the score sign before returning it.
%

%
This algorithm does not care about image orientation. For instance, it could choose an image that
does not include the robot from its point of view. This limit makes the algorithm good only for
quite linear route taken by the robot, but bad for more complex ones. On the other hand, its
implementation is very simple.
%

%
The \texttt{Sweep Metric Algorithm} in chapter \ref{subsec:sweep_metric_algorithm} will try to
go behind this limits.


\subsubsection{The sweep metric algorithm}
\label{subsec:sweep_metric_algorithm}
The "sweep angle" algorithm allows to choose the better background image, in order to implement
the exocentric vision.
%
\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=400pt]{img/half_plan_finding.png} 
    \caption{Sweep Angle Algorithm}
    \label{fig:half_plan_finding}
  \end{center}
\end{figure}
%
All the explanation will refer to image \ref{fig:half_plan_finding}. In the latter, the black
square represents the robot, with its orientation indicated by the arrow "a", starting from the
square.
%

%
The several blue circles represent instead the previous shoot images. The orientation of each
image (i.e. the orientation of the robot when they were shoot) is shown by an arrow starting from
each circle.
%

%
We will refer to the line perpendicular to the half line "a" as "b". By rotating clockwise and
counterclockwise the "a" line in the robot centre with a predefined angle (named "sweep angle")
we will obtain the "c" and "d" lines. These define a new portion of the plane (coloured with fading red),
named the "sweep area".
%

%
Since we want to control the robot from the rear position, the sought image will be included within
this area: all the other ones will be discarded. Even tough this method allows us to exclude many
images, the selection is certainly not over.
%

%
First of all, we have to discard all the images with an orientation angle that differs too much from the
robot orientation angle. If the difference between the two angles is greater than a specified threshold,
the image can not be chosen as background, because the latter does not include the robot from its point
of view. For instance, if the difference angle between the robot and the image orientation is 180 degrees,
it means that robot and camera are oriented in opposite way, therefore the camera can not see the robot
itself.
%

%
After excluding the badly oriented images, we proceed with the score assignment. The latter is obtained by
the sum of two factors. The first takes in account the image angle orientation: the more the image orientation
angle is close to the robot orientation angle, the more the score is high.
%
\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=400pt]{img/sweep_angle_diagram.jpeg} 
    \caption{Sweep angle diagram}
    \label{fig:sweep_angle_diagram}
  \end{center}
\end{figure}
%
The function to evaluate the score from the difference between the two angles is a Gaussian function centre
in zero. The return value will be therefore always a positive number.
%

%
The use of a Gaussian function allows to obtain different values even for two very close angle difference
(Gaussian is a injective function), regardless of the assigned variance. Moreover, it is defined for all real
numbers.
In this way is always possible to discern the best angle difference, by choosing the highest value returned by
the function.
%

%
Finally, we have to take in account the euclidean distance between the image and the robot. If the distance is
zero, we are examining the egocentric image. On one side, the images too close to the robot will be coupled
with a low score, because they are too similar to the egocentric one. On the other side, the images too far
from the robot will be coupled again with a low score, because they would show the robot too distance to
teleguide it properly.
%

%
The preferable image would distance from the robot a predefined quantity, specified by the user. This value
indicate us where to centre another Gaussian function, in order to obtain the score from the distance between
image and robot. The more the image is close to the preferred distance, the more the score assigned will be high.
%

%
The reasons that justifies the use of the Gaussian function are the same previously explained for the angle
difference. The values obtained from the two Gaussian functions, after adding them, form the global score.
%

%
To summarize the method above described (illustrated in figure \ref{fig:sweep_angle_diagram}), we coupled
every image with a score: the one with the maximum value will be chosen and put as background. If the image is
not included in the sweep area (defined by a sweep angle and therefore by the "c" and "d" line) it has to be
discarded. The score assigned will be -2.
%

%
The images situated within the sweep area, but with an orientation angle that differs too much from the robot
orientation angle, have to be discarded as the previous one: the score assigned will be -2 again.
If the image position and orientation coincide with robot position and orientation, the image represents the
egocentric vision. We remember that in our set there will always be the egocentric image, and that this will be
choose as background when no other image for exocentric vision are available. The score coupled with the egocentric 
vision will be -1.
%

%
Finally, all the other images - if present - are coupled with a positive number, because obtained by the sum of
values calculated with Gaussian functions. The most high score indicate the better image to chose as background;
if there are no preferable images, the image with -1 (i.e. the egocentric image) will be chosen.
%

%
Because the \texttt{ChooseImage} method is designed to choose the image with the minimum score and not with maximum,
all we have to do is multiply every score for -1 before returning it: in this way we convert a maximum research in
a minimum one.
%

%
\paragraph{The WithinBoundaries algorithm}
\label{par:withinboundaries}

Checking if an image is included in the "sweep angle" (see chapter \ref{subsec:sweep_metric_algorithm} for its
definition) could be something not too easy to implement. Given image coordinates, robot coordinates and robot
orientation, the \texttt{WithinBoundaries} method must replays (with a true or false response) to the question
"is the image included in the sweep area ?", shown in previous diagram (figure \ref{fig:sweep_angle_diagram}).
%

%
Since the area defined by the "sweep angle" depends on robot coordinates and orientation, we should insert in
the source code a lot of geometrical and mathematical rules, frequently not immediately comprehensible
(even for the authors). For this reason we decided to use a different approach.
%

\begin{figure}[htp]
  \begin{center}
    \subfigure[Initial robot and image coordinates]{
      \label{fig:sweepal_start}\includegraphics[width=175pt, height=175pt]{img/sweepal_start.png}
    }
    \hspace*{15pt}
    \subfigure[Translated system]{
      \label{fig:sweepal_translate}\includegraphics[width=175pt, height=175pt]{img/sweepal_translate.png}
    }

    \subfigure[Rotated system]{
      \label{fig:sweepal_rotate}\includegraphics[width=175pt, height=175pt]{img/sweepal_rotate.png}
    } 
    \hspace*{15pt}
    \subfigure[Triangle AOB]{
      \label{fig:sweepal_triangle}\includegraphics[width=175pt, height=175pt]{img/sweepal_triangle.png}
    } 

    \vspace*{20pt}
    \subfigure[Symbol definition]{
      \label{fig:sweepal_caption}\includegraphics[width=255pt, height=65pt]{img/sweepal_caption.png}
    } 

  \end{center}
  \caption{WithinBoundaries algorithm}
  \label{fig:withingboundaries}
\end{figure}

%
Before checking if a point (i.e. an image) is included in the "sweep angle", the robot and image coordinates
are translated and then rotated, in order to move the robot in the origin of the axis and to overlap its
orientation arrow with the y-axis. The two transformations are shown in figure \ref{fig:sweepal_translate} and
\ref{fig:sweepal_rotate}, while the robot and image starting coordinates are shown in figure \ref{fig:sweepal_start}.
%

%
After executing these transformations, the sweep area is always situated in the second and third quadrant, as
shown in figure \ref{fig:sweepal_rotate}. Now the "c" and "d" lines pass both from the origin. For an exhaustive
definition of line "c" and "d" see chapter \ref{subsec:sweep_metric_algorithm}.
%

%
If we draw a circle centred in the axis origin, the intersection between the circle and the "c" and "d" lines will
return four points in the plan. Among these, the points situated in the second and third quadrant define a triangle
with the centre of the XY axis: this will be our "sweep area", where to check if an image is included or not. Note
that we choose the points with negative Y value (named "A" and "B") due to the translation and rotation operated
before. Again, see figure \ref{fig:sweepal_triangle} for a graphical example.
%

%
The circle radius must be a value large enough to include a wide number of images. In our case we defined it with a
value of five hundred, in order to reduce the difference between the abstract "sweep area" and the practical triangle
"AOB" (figure \ref{fig:sweepal_triangle}) used to simplify the algorithm.
%

%
Defined the "AOB" triangle, we have to cope with a well-known problem: checking if a point is included or not in a
triangle. The better (and more performant) way to resolve the problem exploits the cross product between vectors
in three-dimensional Euclidean space.
%

%
\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=300pt]{img/sweepal_crossproductABC.jpeg} 
    \caption{Cross product between vectors}
    \label{fig:sweepal_crossproductABC}
  \end{center}
\end{figure}
%

Referring to the image \ref{fig:sweepal_crossproductABC}, the cross product of [A-B] and [A-p] will result a vector
pointing out of the screen. On the other hand, the cross product of [A-B] and [A-p'] will result a vector pointing
into the screen.
%

%
The cross product of [A-B] with the vector from A to any point above the segment AB turns out with a resulting vector
points out of the screen, while using any point below AB yields with a vector pointing into the screen. We have to
distinguish which direction a resulting vector must have in order to consider the point "p" inside the triangle.
%

%
Because the triangle can be oriented in any way, what we need is a reference point, that is a point that we know is
on a certain side of the line. For our triangle (figure \ref{fig:sweepal_crossproductABC}), this is just the third
point C.
%

%
Any point "p", where [A-B] cross [A-p] does not point in the same direction as [A-B] cross [A-C], is not inside the
triangle. If the cross products do point in the same direction, then we need to test "p" with the other lines as well.
If the point was on the same side of AB segment as C, and is also on the same side of BC segment as A, and on the same
side of CA segment as B, then it is in the triangle.
%

%
The chief disadvantage regarding the approach above described is that the sweep angle value must be strictly greater
than zero and strictly less than ninety degrees. If the sweep angle exceeds previous limits the algorithm will executes
with a wrong triangle AOB (we remember that AOB angle, shown in figure \ref{fig:sweepal_triangle}, is equal to twice
the sweep angle).
%

%
The described procedure, whose aim is knowing if a point is included within a triangle, can be found in reference
\cite{withinboundaries:pointintriangle}. Its result is in the end the sought answer to the beginning question "is the
image included in the sweep area ?" (figure \ref{fig:sweep_angle_diagram}) and will be returned by the
\texttt{WithinBoundaries} method by means of a boolean value.
%
