\section{Setting up a Morduc simulator}
\label{sec:simulator}
A basic exocentric vision system for Morduc would need, 
at least, some images captured from the camera mounted 
on top of it and, for each image, the actual position of 
the robot at the time it was captured.
%
While images could be used as a texture on which draw 
a 3d represenation of the robot, informations about 
position are essential in order to draw the robot 
consistently, i.e. in the position the observer would 
see it if he was seeing the robot by means of an actual 
external camera.
%

%
An exocentric vision systems performs at its best 
when there's a large static space where the robot can move in. Since 
the real robot can not be easily teleguided in wide 
environment because of its size and the lack of a large 
room in Catania, where the robot is situated, a simulator 
has been used to reproduce the best set of data. The 
simulator can be thought of as a server, which receives 
requests and returns responses: the first are the commands 
sent by the user to move the robot, the latter the 
egocentric images and the robot position data.
%

%
Furthermore, with a simulator is extremely easy to 
change the environment where the robot is teleguided, 
so we can test the exocentric vision with an infinite 
number of environments without physically moving the 
robot in different places. In this way software 
development of exocentric vision can be faster, because 
it is simple and immediate to establish several test cases.
%

%
The first simulator adopted was Rosen\cite{rosen}. Written in 
Erlang \cite{erlang}, Rosen has been developed at the University
of Catania in order to simulate the behavior of robot with 
features completely different from Morduc. The chief one 
was that the robot had its own algorithm to set its movement, 
it was indeed not guided by a human operator. 
%

%
Rosen was used a couple of years ago for robot taking part 
in Eurobot competition\cite{eurobot},
but for our purpose was not fit since it does not even 
provide a egocentric vision. The difficulties faced to edit 
Rosen made us to look for a more suitable simulator.
%

%
In 2006, at the Aalborg University, the Italian student Filippo 
Privitera wrote a simulator to teleguide the Morduc robot 
\cite{privitera}. The simulator reproduces the robot (drawing its 
physical features) situated in a room with a variable numbers 
of walls. Walls' position is specified by the user, by giving 
the simulator a black and white bitmap image with the room 
plane to build the whole environment from. 
%

%
Besides, Privitera's simulator allows to enable the 
stereoscopic vision, with anaglyph or polarized method (both 
types are applied on the egocentric camera). These features 
make not too tricky to implement the exocentric vision control 
with 3d effect, in order to improve the user's ability in 
moving the robot. Other information like the number of collisions 
or the robot distance from the nearest obstacle are provided 
by simulator.
%

%
This simulator was written in MFC (Microsoft Foundation Class) 
framework and has been used for testing user ability in
driving the robot, in comparison with the real robot server 
to quantify the differences between the two facilities. 
For more information see \cite{privitera}.
%

%
With a simulator specifically built for the Morduc robot, it 
was not difficult to edit the source code to obtain what we 
needed. First of all, we needed to record data about egocentric 
vision and robot status, because they are the input value 
of the exocentric vision simulator. In order to achieve this, 
we edited the source code to allows the user to store data: by 
pressing the 'P' key keyboard the actual information (e.g. the 
actual camera image and robot status) are recorded in log files.
%

%
Every session in Privitera's simulator has its own identifier 
(a integer number). When the 'P' key is pressed the simulator 
write a new line in the text file named 'data\_$\langle$number 
of session$\rangle$.txt', creating the file if it does not 
exist. Each line contains four float number values, with the 
following meaning:
%
\begin{enumerate}
\item x coordinate
\item y coordinate
\item theta value (in radiant s)
\item timestamp
\end{enumerate}
%
where the timestamp refers to the beginning of the simulation.
%

%
Beside the text file there are several PNG files, each for every 
line written in 'data\_$\langle$number of session$\rangle$.txt'. 
These files are named 'screenshot\_$\langle$number of 
session$\rangle$\_$\langle$timestamp$\rangle$.png', where the 
number of session indicates for every screenshot 
- i.e. the egocentric vision - the proper text file, and the 
timestamp the line with the associated status of the robot.
%

%
The software which implement the exocentric vision control 
will look for text and image files related to a specific 
session, in order to read the necessary input and draw the 
robot correctly. It must be able to choose the right image 
to use as background, among those previously read; to draw 
the robot over the background in the right position and 
orientation, depending on its route; to prevent or signal 
collisions to the user, and so on.
%

%
Future development can reach a more interactive collaboration 
between simulator and exocentric vision control program. For
example, it would be better if the control program could 
interact (by a socket or another data stream) with the 
simulator, sending the movement command and retrieving 
information about robot status. In this case log files 
would be substituted by a local or global connection, 
and the exocentric program would not read information from 
file (as it actually does) but from the network.
Furthermore, with log files the robot route is statically 
decided when they are created. With a network connection user
could teleguide the robot in real time, without caring 
if on the server side is present the simulator or the real 
robot Morduc.
